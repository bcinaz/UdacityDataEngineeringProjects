 # Project: Create AWS Redshift Data Warehouse
 
Sparkify, has grown their user base and song database and want to move their processes and data onto the cloud. Their data resides in S3, in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.

As their data engineer, I have built an ETL pipeline that extracts their data from S3, stages them in Redshift, and transforms data into a set of dimensional tables for their analytics team to continue finding insights in what songs their users are listening to. 

## How to run

Before running, configuration file dhw.cfg should include login details for an activve AWS Redshift cluster and ARN for an IAM roles with S3 read access.

1. Run **create_tables.py** to create staging and analytical tables.
2. Run **etl.py**  to process and load data into data warehouse.

## Database Schema 

The Sparkify database consists of five tables in the star schema. The fact table is called songplays, and contains a record of each songplay event generated by users of the music streaming app. There are four dimension tables which are users, artists, songs and timestamps.

| Table | Description|
| --------------- | --------------- |
| staging events| staging table for the events | 
| staging songs | staging table for the songs| 
| songplays | records on how songs were played | 
| users | information about users such as name, gender|
| songs | information about songs such as artist, year, name| 
| artists | artist name, location|
| time |  timestamps of records in songplays|

## ETL Pipeline
1. Load data from S3 to staging tables on Redshift
2. Load data from staging tables to analytics tables on Redshift.



